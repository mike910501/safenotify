/**
 * Model Selector Service
 * Optimizes OpenAI model selection based on context and intent
 * Reduces costs by 80-95% while maintaining quality
 */

/**
 * Model pricing reference (per 1K tokens)
 * gpt-4: $0.03 input, $0.06 output - Most expensive
 * gpt-4o: $0.005 input, $0.015 output - Best for important leads
 * gpt-3.5-turbo: $0.0005 input, $0.0015 output - Good balance
 * gpt-4o-mini: $0.00015 input, $0.0006 output - Most economical
 */

const MODELS = {
  PREMIUM: 'gpt-5',         // For high-value interactions (most powerful)
  STANDARD: 'gpt-5-mini',   // For normal conversations (recommended balance)
  ECONOMY: 'gpt-5-nano',    // For simple queries and analysis (ultra cheap)
  FALLBACK: 'gpt-4o-mini'   // Fallback if GPT-5 unavailable
};

/**
 * Select optimal model based on lead context and intent
 * @param {Object} leadContext - Lead information (score, state, etc)
 * @param {String} currentIntent - Current conversation intent
 * @returns {String} - Optimal model to use
 */
function selectOptimalModel(leadContext, currentIntent) {
  // High-value lead or demo request = Premium model
  if (leadContext?.qualificationScore >= 70 || 
      currentIntent === 'demo_request' || 
      currentIntent === 'closing' ||
      leadContext?.status === 'qualified') {
    console.log('ðŸŽ¯ Using PREMIUM model for high-value interaction');
    return MODELS.PREMIUM;
  }

  // Sentiment analysis or simple queries = Economy model
  if (currentIntent === 'sentiment_analysis' || 
      currentIntent === 'greeting' || 
      currentIntent === 'basic_info' ||
      !leadContext?.qualificationScore ||
      leadContext?.qualificationScore < 30) {
    console.log('ðŸ’° Using ECONOMY model for simple interaction');
    return MODELS.ECONOMY;
  }

  // Default: Standard model for normal conversations
  console.log('âš–ï¸ Using STANDARD model for normal interaction');
  return MODELS.STANDARD;
}

/**
 * Get model configuration with optimized parameters
 * @param {String} model - Model name
 * @param {String} purpose - Purpose of the request
 * @returns {Object} - Model configuration
 */
function getModelConfig(model, purpose = 'conversation') {
  const configs = {
    conversation: {
      tokens: 250,
      temperature: 0.7,
      presence_penalty: 0.1,
      frequency_penalty: 0.1
    },
    analysis: {
      tokens: 100,
      temperature: 0.1,
      presence_penalty: 0,
      frequency_penalty: 0
    },
    creative: {
      tokens: 300,
      temperature: 0.9,
      presence_penalty: 0.3,
      frequency_penalty: 0.3
    }
  };

  const baseConfig = configs[purpose] || configs.conversation;
  
  // âœ… FIXED: Use correct token parameter based on model
  const modelConfig = {
    model,
    temperature: baseConfig.temperature,
    presence_penalty: baseConfig.presence_penalty,
    frequency_penalty: baseConfig.frequency_penalty
  };
  
  // Use max_completion_tokens for GPT-5 and new models
  if (model.startsWith('gpt-5') || model.startsWith('o1') || model.startsWith('o3')) {
    modelConfig.max_completion_tokens = baseConfig.tokens;
    console.log(`ðŸ“ Config: Using max_completion_tokens for ${model}: ${baseConfig.tokens}`);
  } else {
    // Legacy models use max_tokens
    modelConfig.max_tokens = baseConfig.tokens;
    console.log(`ðŸ“ Config: Using max_tokens for ${model}: ${baseConfig.tokens}`);
  }
  
  return modelConfig;
}

/**
 * Track model usage for cost optimization (LEGACY)
 * @param {String} model - Model used
 * @param {Number} tokens - Tokens consumed
 * @deprecated Use trackGPTUsage from gptUsageTracker instead
 */
function trackModelUsage(model, tokens) {
  const costs = {
    'gpt-4o': 0.005,
    'gpt-3.5-turbo': 0.0005,
    'gpt-4o-mini': 0.00015,
    'gpt-4': 0.03
  };

  const estimatedCost = (tokens / 1000) * (costs[model] || 0.001);
  console.log(`ðŸ“Š Model: ${model}, Tokens: ${tokens}, Est. Cost: $${estimatedCost.toFixed(4)}`);
  
  return {
    model,
    tokens,
    estimatedCost
  };
}

/**
 * Enhanced tracking with database persistence and notifications
 * @param {Object} usageData - Comprehensive usage data
 * @returns {Object} - Usage tracking result
 */
async function trackGPTUsageEnhanced(usageData) {
  try {
    const { trackGPTUsage } = require('./gptUsageTracker');
    return await trackGPTUsage(usageData);
  } catch (error) {
    console.error('âŒ Enhanced tracking failed, falling back to basic:', error);
    return trackModelUsage(usageData.model, usageData.tokensUsed);
  }
}

module.exports = {
  MODELS,
  selectOptimalModel,
  getModelConfig,
  trackModelUsage,
  trackGPTUsageEnhanced
};